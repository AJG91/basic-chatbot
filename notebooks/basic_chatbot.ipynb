{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f6d6fe-b7f0-4e52-88bf-6aaf84d4674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from basic_chatbot.model import LocalLM\n",
    "from basic_chatbot.prompt_utils import build_prompt, extract_assistant_reply\n",
    "from basic_chatbot.utils import set_seed, generate_namespace\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97740a42-b9fd-43de-907b-9e503ec9f558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model_name\": \"gpt2\",\n",
      "  \"seed\": 42,\n",
      "  \"fig_path\": \"../outputs/\",\n",
      "  \"res_path\": \"../results/\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cfg = generate_namespace(path=f\"../config.yaml\")\n",
    "print(json.dumps(vars(cfg), indent=2))\n",
    "\n",
    "set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1e0788c-d364-4a8a-b3f3-cd3ab0e85929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm David Stankiewicz and this is my friend John Condon...I have been working in the tech industry for about 10 years now so we know how to do things well together but they don't always work out just right....you need some kind of communication with them.\"\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "lm = LocalLM(cfg.model_name)\n",
    "\n",
    "def chat(\n",
    "    message: str,\n",
    "    inst,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    prompt = build_prompt(message)\n",
    "    reply = inst.evaluate_text(prompt)\n",
    "    return extract_assistant_reply(reply)\n",
    "\n",
    "resp = chat(\"Hello! Who are you?\", lm)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b99d0c-8bca-49a2-8dac-1c58a01970cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     user = input(\"You:\").strip()\n",
    "#     if user.lower() in {\"exit\", \"quit\"}:\n",
    "#         break\n",
    "#     print(\"Assistant:\", chat(user, lm))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c58d6-e7ba-43b1-b3da-967792ed30ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
