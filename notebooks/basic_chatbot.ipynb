{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f6d6fe-b7f0-4e52-88bf-6aaf84d4674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from chatbot.model import CausalTextGeneration\n",
    "from chatbot.utils import to_device, get_device, set_seed, generate_namespace\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97740a42-b9fd-43de-907b-9e503ec9f558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model_name\": \"gpt2\",\n",
      "  \"seed\": 42,\n",
      "  \"fig_path\": \"../outputs/\",\n",
      "  \"res_path\": \"../results/\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cfg = generate_namespace(path=f\"../config.yaml\")\n",
    "print(json.dumps(vars(cfg), indent=2))\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e0788c-d364-4a8a-b3f3-cd3ab0e85929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm going to get your name right now. That's the best way we can help.\"\n",
      "\"I'll make sure it goes straight on,\" said Hermione as she ran out of her room with a smile that looked like someone had punched up in\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    PreTrainedTokenizerBase, PreTrainedModel,\n",
    ")\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "inst = CausalTextGeneration(cfg.model_name)\n",
    "model = inst.load_model()\n",
    "\n",
    "def chat(\n",
    "    prompt: str,\n",
    "    model: PreTrainedModel,\n",
    "    inst,\n",
    "    history: Optional[list[str]] = None\n",
    ") -> Tuple[str, list[str]]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if history is None:\n",
    "        history = []\n",
    "    context = \" \".join(history[-3:]) + \" \" + prompt\n",
    "    inputs = inst.tokenize_text(context)\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "    outputs = inst.evaluate_model(inputs, model, max_new_tokens=50)\n",
    "    reply = inst.decode_tokens(outputs[0][input_length:])\n",
    "    history.append(reply)\n",
    "    return reply, history\n",
    "\n",
    "resp, hist = chat(\"Hello, who are you?\", model, inst)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89880497-799f-4f9e-99da-c42149a4ed4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"You have my permission and you're free to come here, but if there is any questions then please don't ask me about them unless they are important enough for one of us to feel safe at Hogwarts or something!\" She was so pleased when Professor\n"
     ]
    }
   ],
   "source": [
    "resp, hist = chat(\"That's amazing! What else?\", model, inst, hist)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4174926-5baa-47e2-89dc-083c28bfd669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
